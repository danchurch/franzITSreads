## let's see what Franz's reads look like

## put his reads on our beastly, old, slow drive
mv /home/daniel/Downloads/Raw.zip /media/vol/franzData/

unzip Raw.zip

## what are the primers?

## GATGAAGAACGYAGYRAA
## RBTTTCTTTTCCTCCGCT

##     GATGAAGAACG Y  AG Y   R  AA

fprim='GATGAAGAACG[CT]AG[CT][AG]AA'

## the full IUPAC reverse complement would be:
'TTYRCTRCGTTCTTCATC'
## so...
fprimRC='TT..CT.CGTTCTTCATC'

##      R    B  TTTCTTTTCCTCCGCT
rprim='[AG][GCT]TTTCTTTTCCTCCGCT'

## the full reverse complement IUPAC would be:

## or
#rprim='..TTTCTTTTCCTCCGCT'

rprimRC='AGCGGAGGAAAAGAAA..'

## look for these
cd /media/vol/franzData/Raw/FITS2/EC_2021_080_057_F_ITS_E192_L001-ds.c5b0e235b400480c9f7e35d36a59096f

zgrep --color $rprim *R1*
zgrep --color $rprim *R2*
zgrep --color $fprim *R1*
zgrep --color $fprim *R2*

## seems consistent, assume this is the case for all the files, R2 has the forward primer-linked sequences,
## R1 has the reverse.

## let's try cutadapt.

## following the cutadapt manual (https://cutadapt.readthedocs.io/en/stable/recipes.html)

conda activate cutadapt

cd /media/vol/franzData/Raw/FITS2
outdir="/media/vol/franzData/trimmedReads/"
#mkdir $outdir
for i in *; do
  cd $i
  echo $PWD
  ls
  R1=$(ls *R1*.fastq.gz)
  R2=$(ls *R2*.fastq.gz)
  echo ${R1} ${R2}
  echo $outdir${R1} $outdir${R2}
  cutadapt -a RBTTTCTTTTCCTCCGCT...TTYRCTRCGTTCTTCATC \
           -A GATGAAGAACGYAGYRAA...AGCGGAGGAAAAGAAAVY \
           --discard-untrimmed \
           --match-read-wildcards \
           -o $outdir${R1} \
           -p $outdir${R2} \
           $R1 $R2
  cd ..
  echo $PWD
  echo "#########################################################"
done

## that gives some empty reads, probably some floating primers
## get rid of these:


cd $outdir
for i in *; do
  echo $i
  seqtk seq -L 50 $i | gzip > ${i}_noshorts
  mv ${i}_noshorts $i
  ls $i
done

## example 

## did that work? Let's assume it did, not much time. 

## moving on with dada2:

R

library(dada2)
library(ShortRead)
library(Biostrings)

plotQualityProfile("/media/vol/franzData/trimmedReads/331_S331_L001_R2_001.fastq.gz")

plotQualityProfile("/media/vol/franzData/trimmedReads/", aggregate=TRUE)


## okay, works. That's what they asked for but now they want to
## run through full dada2 pipeline with me

## that's more work. Let's start a repo:

git clone https://github.com/danchurch/franzITSreads.git

## test

touch thisIsNotReal.txt

git config --global user.email "danchurchthomas@gmail.com"
git config --global user.name "danchurch"
git remote set-url origin git@github.com:danchurch/franzITSreads.git

## so, franz would love it if everything were done in R.



## anyway they have an example script:
https://benjjneb.github.io/dada2/ITS_workflow.html

## they want all files in the same directory. Franz said to take files
## only up to a point, the first 39 plots or so


cd /media/vol/franzData/Raw/FITS2

## just to understand the file names:

## each pair is in a folder
EC_2021_080_001_FITS_E176_L001-ds.b363ac7ea0564138bb1452757e403ce7 ## first samples
## the filenames then look like this:
193_S193_L001_R1_001.fastq.gz  193_S193_L001_R2_001.fastq.gz

EC_2021_080_039_FITS_E176_L001-ds.977b416e0afc478fa6a786e82c4370e3 ## last paired samples, #39
231_S231_L001_R1_001.fastq.gz  231_S231_L001_R2_001.fastq.gz

## so the information we are interested in is in the folder names, not the read files
## the sample number is the first number after "080" in the folder name. 
## but it looks like we can recover the sample from the sequencer sample number in the read file names,
## by subtracting 192 from the first number in each read name.

## so okay to combine them all into a folder, we can recover the sample from them.

cd /media/vol/franzData/Raw/FITS2

for i in {1..9}; do
  echo $i
  cp EC_2021_080_00${i}_FITS_E176_L001-ds.*/* /media/vol/franzData/rawReadsReorganized/
  #cd ..
done

for i in {10..39}; do
  echo $i
  cp EC_2021_080_0${i}_FITS_E176_L001-ds.*/* /media/vol/franzData/rawReadsReorganized/
done


## from there I think we can follow the tutorial in R
https://benjjneb.github.io/dada2/ITS_workflow.html

library(dada2)
library(ShortRead)
library(Biostrings)
library(phyloseq)
library(ggplot2)

path <- "/media/vol/franzData/rawReadsReorganized" 

list.files(path)

## above we noted that the sequences with the forward primer are the R2:
fnFs <- sort(list.files(path, pattern = "R2_001.fastq.gz", full.names = TRUE))
fnRs <- sort(list.files(path, pattern = "R1_001.fastq.gz", full.names = TRUE))

## primer sequences, with IUPAC ambiguous bases:
FWD <- "GATGAAGAACGYAGYRAA"  
REV <- "RBTTTCTTTTCCTCCGCT"  

allOrients <- function(primer) {
    # Create all orientations of the input sequence
    require(Biostrings)
    dna <- DNAString(primer)  # The Biostrings works w/ DNAString objects rather than character vectors
    orients <- c(Forward = dna, Complement = Biostrings::complement(dna), Reverse = Biostrings::reverse(dna),
        RevComp = Biostrings::reverseComplement(dna))
    return(sapply(orients, toString))  # Convert back to character vector
}

FWD.orients <- allOrients(FWD)
REV.orients <- allOrients(REV)

FWD.orients
REV.orients

fnFs.filtN <- file.path(path, "filtN", basename(fnFs)) # Put N-filtered files in filtN/ subdirectory
fnRs.filtN <- file.path(path, "filtN", basename(fnRs))

## get rid of N-labeled nucleotides
filterAndTrim(fnFs, fnFs.filtN, fnRs, fnRs.filtN, maxN = 0, multithread = TRUE)

## look for our primers in the reads, using essentially the vcountPattern function from biostrings
primerHits <- function(primer, fn) {
    # Counts number of reads in which the primer is found
    nhits <- vcountPattern(primer, sread(readFastq(fn)), fixed = FALSE)
    return(sum(nhits > 0))
}

## make a table of which primers are found where. 

rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs.filtN[[1]]), 
      FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = fnRs.filtN[[1]]), 
      REV.ForwardReads = sapply(REV.orients, primerHits, fn = fnFs.filtN[[1]]), 
      REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs.filtN[[1]]))

## makes sense. Forward primers are where they expected, RC primers same

## check cut adapt path. Might be weird because I'm in a conda env
cutadapt <- "/home/daniel/miniconda3/envs/cutadapt/bin/cutadapt" # cutadapt path, in my case in a conda path
system2(cutadapt, args = "--version") # Run shell commands from R

## start prepping the command for cutadapt:

path.cut <- file.path(path, "cutadapt")

if(!dir.exists(path.cut)) dir.create(path.cut)
fnFs.cut <- file.path(path.cut, basename(fnFs))
fnRs.cut <- file.path(path.cut, basename(fnRs))


FWD.RC <- dada2:::rc(FWD)
REV.RC <- dada2:::rc(REV)
# Trim FWD and the reverse-complement of REV off of R1 (forward reads)
R1.flags <- paste("-g", FWD, "-a", REV.RC) 
# Trim REV and the reverse-complement of FWD off of R2 (reverse reads)
R2.flags <- paste("-G", REV, "-A", FWD.RC) 


# Run Cutadapt
for(i in seq_along(fnFs)) {
  system2(cutadapt, args = c(R1.flags, R2.flags, "-n", 2, # -n 2 required to remove FWD and REV from reads
                             "-o", fnFs.cut[i], "-p", fnRs.cut[i], # output files
                             "-m", 20, # enforce a minimum length
                             fnFs.filtN[i], fnRs.filtN[i])) # input files
}

## check that it worked:

rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs.cut[[1]]), 
      FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = fnRs.cut[[1]]), 
      REV.ForwardReads = sapply(REV.orients, primerHits, fn = fnFs.cut[[1]]), 
      REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs.cut[[1]]))

## gone, awesome

# Forward and reverse fastq filenames have the format:
cutFs <- sort(list.files(path.cut, pattern = "R2_001.fastq.gz", full.names = TRUE))
cutRs <- sort(list.files(path.cut, pattern = "R1_001.fastq.gz", full.names = TRUE))

# Extract sample names, for the moment let's use the sample numbers given by the sequencer

get.sample.name <- function(fname) strsplit(basename(fname), "_")[[1]][1]
sample.names <- unname(sapply(cutFs, get.sample.name))

plotQualityProfile(cutFs[1:2])
## huh, these don't look amazing. At least they have enough depth.

################################################################################
## side note, I don't really like their plotter. 
## Let's look at these outside of R in good old fashioned fastqc 
cp /media/vol/franzData/rawReadsReorganized/cutadapt/* /media/vol/franzData/rawReadsReorganized/concatted4fastqc/
cd /media/vol/franzData/rawReadsReorganized/concatted4fastqc/
gunzip *
cat *R1* | gzip > oneBigFastqFranzITS_R1_reads.fastq.gz
cat *R2* | gzip > oneBigFastqFranzITS_R2_reads.fastq.gz
rm *fastq
fastqc oneBigFastqFranzITS_R1_reads.fastq.gz -o /media/vol/franzData/franzITSreads/
fastqc oneBigFastqFranzITS_R2_reads.fastq.gz -o /media/vol/franzData/franzITSreads/
cd /media/vol/franzData/franzITSreads
firefox *html
## okay, that is nice to look at. 
################################################################################

## anyway, on down the pipeline

filtFs <- file.path(path.cut, "filtered", basename(cutFs))
filtRs <- file.path(path.cut, "filtered", basename(cutRs))

## on the tutorial they do an additional filtering step. Don't know why
## they don't do this earlier.

## the settings used in the tutorial for the 
## following command cause the vast majority to
## be thrown out. So I relaxed the max expected
## error from 2 to 5. We still lose average of ~50%

out <- filterAndTrim(cutFs, filtFs, cutRs, filtRs, 
    maxN = 0, 
    maxEE = c(5, 5), 
    truncQ = 2,
    minLen = 50, 
    rm.phix = TRUE,
    compress = TRUE, 
    multithread = TRUE)  

out

colSums(out) ## 1416136 in, 815679 out

errF <- learnErrors(filtFs, multithread = TRUE)
errR <- learnErrors(filtRs, multithread = TRUE)

plotErrors(errF, nominalQ = TRUE)

plotErrors(errR, nominalQ = TRUE)

## try pooled. If it takes to long, move to lab computer?
dadaFs <- dada(filtFs, err = errF, multithread = TRUE, pool=TRUE) ## ~20 min, 10 gig RAM
dadaRs <- dada(filtRs, err = errR, multithread = TRUE, pool=TRUE) ##

dadaRs

mergers <- mergePairs(dadaFs, filtFs, dadaRs, filtRs, verbose=TRUE)

seqtab <- makeSequenceTable(mergers)

?makeSequenceTable

dim(seqtab)

seqtab[1:3,1:2]

sum(seqtab) ##560718 reads. Seems low.

seqtab.nochim <- removeBimeraDenovo(seqtab, method="consensus", multithread=TRUE, verbose=TRUE)

## check the variability in sequence length:
hist(nchar(getSequences(seqtab.nochim)), breaks=20)

getN <- function(x) sum(getUniques(x))

track <- cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN), sapply(mergers, getN),
    rowSums(seqtab.nochim))

colnames(track) <- c("input", "filtered", "denoisedF", "denoisedR", "merged", "nonchim")
rownames(track) <- sample.names

track

## side note, prepping for taxonomic id

## I downloaded UNITE sh_general_release_04.04.2024.tgz
## as available from:
## https://doi.plutof.ut.ee/doi/10.15156/BIO/2959332
## https://doi.org/10.15156/BIO/2959332

## the general page is here: https://unite.ut.ee/repository.php

## this is version 10.0, dynamic, non-dev version with fewer singletons.

unite.ref <- "/media/vol/franzData/sh_general_release_dynamic_04.04.2024.fasta"  # CHANGE ME to location on your machine

## start time is 2024-07-29 17:10
taxa <- assignTaxonomy(seqtab.nochim, unite.ref, multithread = TRUE, tryRC = TRUE)

print(paste("finish time is", Sys.time())) ## that took maybe 15 min, 15 gig RAM

## okay, finish this up, get to a phyloseq object with rep seqs and taxonomy and metadata

taxa.print <- taxa  # Removing sequence rownames for display only
rownames(taxa.print) <- NULL

head(taxa.print) 

## we don't have a metadata file, but can get some information from 
samples.out <- rownames(seqtab.nochim)

## theoretically, we can just take the order of these samples as their sample number,
## but not very robust, so make a function that gets the sample out of the name:

getSampNu <- function(x){paste0("S", as.character(as.integer(strsplit(x, "_")[[1]][1]) - 192))}

## use these as new rownames
rownames(seqtab.nochim) <- sapply(samples.out, getSampNu)

seqtab.nochim[1:5,1:2]

## make our phyloseq object:
ps <- phyloseq(otu_table(seqtab.nochim, taxa_are_rows=FALSE), 
               tax_table(taxa))

## we would normally add environmental data here

## change out the repseqs for ASV name/numbers

dna <- Biostrings::DNAStringSet(taxa_names(ps))
names(dna) <- taxa_names(ps)
ps <- merge_phyloseq(ps, dna)
taxa_names(ps) <- paste0("ASV", seq(ntaxa(ps)))
otu_table(ps)[1:5,1:5]

## should be ready for ordinations, etc. But we need environmental data. 


############## co1 pipeline ####################

## I signed to make the co1 pipeline community matrix

## seems like this is the paper to look at:
https://www.nature.com/articles/s41598-018-22505-4     
## (Porter and Hajibabaei 2018)

## github repo here: 

## dada3 github issue mentioning this tool
https://github.com/benjjneb/dada2/issues/922

## looks like this will be a task. General approach would be
## get the denoising then branch off to other software
## for the taxonomic identification, then move back into 
## pipeline for phyloseq object:

## first step, as always, find the primers:

## forward:
GGWACWGGWTGAACWGTWTAYCCYCC

## reverse
TAAACTTCAGGGTGACCAAARAAYCA

## greppable:
fprim="GG.AC.GG.TGAAC.GT.TA.CC.CC"
rprim="TAAACTTCAGGGTGACCAAA.AA.CA"

cd /media/vol/franzData/Raw/CO1

cd EC_2021_080_048_L001-ds.06b957a958394972b9c8b8ef19a6b385

zgrep --color $fprim *R1*  ## nope
zgrep --color $rprim *R1* ## lots
zgrep --color $fprim *R2* ## lots
zgrep --color $rprim *R2* ## one, weird

## so, 


## as before, flatten directory, again only the first 39 samples:

## empty our old working directory and start again CO1 reads

#rm -r /media/vol/franzData/rawReadsReorganized/
mkdir -p /media/vol/franzData/rawReadsReorganized/

cd /media/vol/franzData/Raw/CO1

for i in {1..9}; do
  echo $i
  cp EC_2021_080_00${i}_CO1_E176_L001-ds.*/* /media/vol/franzData/rawReadsReorganized/
done

for i in {10..39}; do
  echo $i
  cp EC_2021_080_0${i}_CO1_E176_L001-ds.*/* /media/vol/franzData/rawReadsReorganized/
done

cd /media/vol/franzData/rawReadsReorganized/

## check again with all sequences
zgrep -c --color $fprim *R1*  ## nope
zgrep -c --color $rprim *R1* ## lots
zgrep -c --color $fprim *R2* ## lots
zgrep -c --color $rprim *R2* ## one, weird

## and then back on the R pipeline:

library(dada2)
library(ShortRead)
library(Biostrings)
library(phyloseq)
library(ggplot2)


path <- "/media/vol/franzData/rawReadsReorganized" 

list.files(path)

## above we noted that the sequences with the forward primer are the R2:

fnFs <- sort(list.files(path, pattern = "R2_001.fastq.gz", full.names = TRUE))

fnRs <- sort(list.files(path, pattern = "R1_001.fastq.gz", full.names = TRUE))

FWD <- "GGWACWGGWTGAACWGTWTAYCCYCC"  
REV <- "TAAACTTCAGGGTGACCAAARAAYCA"  

allOrients <- function(primer) {
    # Create all orientations of the input sequence
    require(Biostrings)
    dna <- DNAString(primer)  # The Biostrings works w/ DNAString objects rather than character vectors
    orients <- c(Forward = dna, Complement = Biostrings::complement(dna), Reverse = Biostrings::reverse(dna),
        RevComp = Biostrings::reverseComplement(dna))
    return(sapply(orients, toString))  # Convert back to character vector
}

fnFs.filtN <- file.path(path, "filtN", basename(fnFs)) # Put N-filtered files in filtN/ subdirectory
fnRs.filtN <- file.path(path, "filtN", basename(fnRs))


## get rid of N-labeled nucleotides
filterAndTrim(fnFs, fnFs.filtN, fnRs, fnRs.filtN, maxN = 0, multithread = TRUE)

## look for our primers in the reads, using essentially the vcountPattern function from biostrings
primerHits <- function(primer, fn) {
    # Counts number of reads in which the primer is found
    nhits <- vcountPattern(primer, sread(readFastq(fn)), fixed = FALSE)
    return(sum(nhits > 0))
}


## make a table of which primers are found where. 

FWD.orients <- allOrients(FWD)
REV.orients <- allOrients(REV)

rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs.filtN[[1]]), 
      FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = fnRs.filtN[[1]]), 
      REV.ForwardReads = sapply(REV.orients, primerHits, fn = fnFs.filtN[[1]]), 
      REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs.filtN[[1]]))

## check cut adapt path. Might be weird because I'm in a conda env
cutadapt <- "/home/daniel/miniconda3/envs/cutadapt/bin/cutadapt" # cutadapt path, in my case in a conda path
system2(cutadapt, args = "--version") # Run shell commands from R

## start prepping the command for cutadapt:

path.cut <- file.path(path, "cutadapt")

if(!dir.exists(path.cut)) dir.create(path.cut)
fnFs.cut <- file.path(path.cut, basename(fnFs))
fnRs.cut <- file.path(path.cut, basename(fnRs))

FWD.RC <- dada2:::rc(FWD)
REV.RC <- dada2:::rc(REV)

# Trim FWD and the reverse-complement of REV off of R1 (forward reads)
R1.flags <- paste("-g", FWD, "-a", REV.RC) 
# Trim REV and the reverse-complement of FWD off of R2 (reverse reads)
R2.flags <- paste("-G", REV, "-A", FWD.RC) 

# Run Cutadapt
for(i in seq_along(fnFs)) {
  system2(cutadapt, args = c(R1.flags, R2.flags, "-n", 2, # -n 2 required to remove FWD and REV from reads
                             "-o", fnFs.cut[i], "-p", fnRs.cut[i], # output files
                             "-m", 20, # enforce a minimum length
                             "-j", 0, ## auto-detect cores
                             "--discard-untrimmed", ## there are 1-2 reads that appear flipped
                             fnFs.filtN[i], fnRs.filtN[i])) # input files
}

## check that it worked:
rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs.cut[[1]]), 
      FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = fnRs.cut[[1]]), 
      REV.ForwardReads = sapply(REV.orients, primerHits, fn = fnFs.cut[[1]]), 
      REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs.cut[[1]]))

## looks clean. 

cutFs <- sort(list.files(path.cut, pattern = "R2_001.fastq.gz", full.names = TRUE))
cutRs <- sort(list.files(path.cut, pattern = "R1_001.fastq.gz", full.names = TRUE))

# Extract sample names, for the moment let's use the sample numbers given by the sequencer
get.sample.name <- function(fname) strsplit(basename(fname), "_")[[1]][1]
## looks like these sequencer names are the Franz-sample number + 96 

sample.names <- unname(sapply(cutFs, get.sample.name))

plotQualityProfile(cutFs[1:2])

pdf('forwardR2readQualitys.pdf')
plotQualityProfile(cutFs[1:12])
dev.off()
## the R1 (reverse) look much better
pdf('reversR1readQualitys.pdf')
plotQualityProfile(cutRs[1:12])
dev.off()

## there is a LOT of variablility of quality among samples. Some are fine,
## others are really nasty. 
## but I always feel this way when I use the dada2 plotter
## look at them with fastqc, and it doesn't looks so bad...

################################################################################
## Let's look at these outside of R in good old fashioned fastqc 
mkdir /media/vol/franzData/rawReadsReorganized/concatted4fastqc/
cp /media/vol/franzData/rawReadsReorganized/cutadapt/* /media/vol/franzData/rawReadsReorganized/concatted4fastqc/
cd /media/vol/franzData/rawReadsReorganized/concatted4fastqc/
gunzip *
cat *R1* | gzip > oneBigFastqFranzCO1_R1_reads.fastq.gz
cat *R2* | gzip > oneBigFastqFranzCO1_R2_reads.fastq.gz
rm *fastq
mkdir /media/vol/franzData/franzCO1reads/
fastqc oneBigFastqFranzCO1_R1_reads.fastq.gz -o /media/vol/franzData/franzCO1reads/
fastqc oneBigFastqFranzCO1_R2_reads.fastq.gz -o /media/vol/franzData/franzCO1reads/
cd /media/vol/franzData/franzITSreads
firefox *html
################################################################################

filtFs <- file.path(path.cut, "filtered", basename(cutFs))
filtRs <- file.path(path.cut, "filtered", basename(cutRs))

## let's get rid R2 (forward, "Fs") reads after 250 BP, the quality is pretty bad after that.
## these reads are lower quality, it is hard to enforce any quality cutoffs
## without losing astronomical amounts of reads...

## the target CO1 region is 313 BP, so we can trim a lot, if this is helpful

out <- filterAndTrim(cutFs, filtFs, cutRs, filtRs, 
    maxN = 0, 
    truncQ = 5,
    truncLen = c(210,260),
    minLen = 50, 
    rm.phix = TRUE,
    compress = TRUE, 
    multithread = TRUE)  


plotQualityProfile(cutFs, aggregate=TRUE)

dev.new()
plotQualityProfile(filtFs, aggregate=TRUE)

## for reverse/R1 reads, keep up to 260 bp:

plotQualityProfile(cutRs, aggregate=TRUE)

plotQualityProfile(filtRs, aggregate=TRUE)

## looks good.

colSums(out) ## 1386093   1382028

## assuming that worked, we need a script,
## because this will need to run through to finish
## while I am away...

print(Sys.time())
errF <- learnErrors(filtFs, multithread = TRUE)
print(Sys.time())
print("########################################################")
print(Sys.time())
errR <- learnErrors(filtRs, multithread = TRUE)
print(Sys.time())

save(errF, file="errF_CO1.rda")
save(errR, file="errR_CO1.rda")

plotErrors(errF, nominalQ = TRUE)

plotErrors(errR, nominalQ = TRUE)

#### finishCO1dada2.r #####

library(dada2)
library(ShortRead)
library(Biostrings)
library(phyloseq)
library(ggplot2)
load(file="/media/vol/franzData/rawReadsReorganized/errF_CO1.rda")
load(file="/media/vol/franzData/rawReadsReorganized/errR_CO1.rda")
setwd("/media/vol/franzData/rawReadsReorganized")
path <- "/media/vol/franzData/rawReadsReorganized"
path.cut <- file.path(path, "cutadapt")
cutFs <- sort(list.files(path.cut, pattern = "R2_001.fastq.gz", full.names = TRUE))
cutRs <- sort(list.files(path.cut, pattern = "R1_001.fastq.gz", full.names = TRUE))
filtFs <- file.path(path.cut, "filtered", basename(cutFs))
filtRs <- file.path(path.cut, "filtered", basename(cutRs))
print(Sys.time())
dadaFs <- dada(filtFs, err = errF, multithread = TRUE, pool=TRUE) 
print(Sys.time())
print("########################################################")
print(Sys.time())
dadaRs <- dada(filtRs, err = errR, multithread = TRUE, pool=TRUE) 
print(Sys.time())
save(dadaFs, file="dadaFs_CO1.rda")
save(dadaRs, file="dadaRs_CO1.rda")
mergers <- mergePairs(dadaFs, filtFs, dadaRs, filtRs, verbose=TRUE)
save(mergers, file="mergersCO1.rda")

#### ^^^finishCO1dada2.r^^^ #####

nohup Rscript finishCO1dada2.r &> finishCO1dada2.log &  ## 

## how does this look? crank up R again

library(dada2)
library(ShortRead)
library(Biostrings)
library(phyloseq)
library(ggplot2)

setwd("/media/vol/franzData/rawReadsReorganized")
load("errR_CO1.rda")
load("errF_CO1.rda")
load("dadaFs_CO1.rda")
load("dadaRs_CO1.rda")
load("mergersCO1.rda")

## recover old variable names:
path <- "/media/vol/franzData/rawReadsReorganized"
path.cut <- file.path(path, "cutadapt")
cutFs <- sort(list.files(path.cut, pattern = "R2_001.fastq.gz", full.names = TRUE))
cutRs <- sort(list.files(path.cut, pattern = "R1_001.fastq.gz", full.names = TRUE))
filtFs <- file.path(path.cut, "filtered", basename(cutFs))
filtRs <- file.path(path.cut, "filtered", basename(cutRs))
get.sample.name <- function(fname) strsplit(basename(fname), "_")[[1]][1] ## sequencer assigned, not Franz
sample.names <- unname(sapply(cutFs, get.sample.name)) ## sequencer assigned, not Franz

## didn't save outputs from filtering, rerun for this dataobject:
out <- filterAndTrim(cutFs, "deleteMe", cutRs, "deleteMe", 
    maxN = 0, 
    truncQ = 5,
    truncLen = c(210,260),
    minLen = 50, 
    rm.phix = TRUE,
    compress = TRUE, 
    multithread = TRUE)  

## and start again
seqtab <- makeSequenceTable(mergers)

dim(seqtab)

seqtab[1:3,1:2]


sample.names <- unname(sapply(cutFs, get.sample.name))

sum(seqtab) ##1125270 reads. Seems low.

seqtab.nochim <- removeBimeraDenovo(seqtab, method="consensus", multithread=TRUE, verbose=TRUE)

hist(nchar(getSequences(seqtab.nochim)), breaks=20)

getN <- function(x) sum(getUniques(x))

track <- cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN), sapply(mergers, getN),
    rowSums(seqtab.nochim))

colnames(track) <- c("input", "filtered", "denoisedF", "denoisedR", "merged", "nonchim")
rownames(track) <- sample.names

track
## mixed, some samples lost a lot of reads, some very few. Corresponds to 
## the mixed qualities we observed above.
 
#### CO1 taxonomy ####

## let's try the midori2 for CO1 taxonomic classification

## we'll use the "unique" version of the CO1-dada2-formatted database
## == MIDORI2_UNIQ_NUC_GB260_CO1_DADA2
## this version includes all unique sequences of CO1 scraped from genbank
## as opposed to "long" version, which takes the longest CO1 sequence 
## available for every NCBI-defined species.
## see section 2.1.4 on the Leray et al 2022 paper.

## on this webpage: https://www.reference-midori.info/download.php
wget https://www.reference-midori.info/download/Databases/GenBank260_2024-04-15/DADA2/uniq/MIDORI2_UNIQ_NUC_GB260_CO1_DADA2.fasta.gz 

midori.ref <- "/media/vol/franzData/midoriDB/MIDORI2_UNIQ_NUC_GB260_CO1_DADA2.fasta"  # CHANGE ME to location on your machine

print(paste("start time is", Sys.time())) ## 2024-08-02 12:48:34
taxa <- assignTaxonomy(seqtab.nochim, midori.ref, multithread = TRUE, tryRC = TRUE)
print(paste("finish time is", Sys.time())) ## memory not enough. port this over to the lab computer:

#saveRDS(seqtab.nochim, file="seqtab_nochim_CO1.rds")

## to recover:
seqtab.nochim <- readRDS("seqtab_nochim_CO1.rds")

### put this on the lab comp (from local)
#getFile="/media/vol/franzData/rawReadsReorganized/seqtab_nochim_CO1.rds"
#putItHere="/media/vol1/daniel/franzMetabarcode"
#scp $getFile test@132.180.112.115:$putItHere

## on lab comp

## preliminary tests show this will take a while. Run it as
## a script:

cd /media/vol1/daniel/franzMetabarcode

###################### co1_assignTax.r ############################

library("dada2")
packageVersion("dada2")
setwd("/media/vol1/daniel/franzMetabarcode")
seqtab.nochim <- readRDS("seqtab_nochim_CO1.rds")
midori.ref <- "/media/vol1/daniel/franzMetabarcode/MIDORI2_UNIQ_NUC_GB260_CO1_DADA2.fasta"
print(paste("start time is", Sys.time())) ## 2024-08-02 14:09:04, taking all (~60gig) RAM. 
taxa <- assignTaxonomy(seqtab.nochim, midori.ref, multithread = TRUE, tryRC = TRUE)
saveRDS(taxa, file="CO1_assTax.rds")
print(paste("finish time is", Sys.time()))

############### ^^^^^co1_assignTax.r^^^^^ #####################

nohup Rscript co1_assignTax.r &> co1_assignTax.log &  ## 

## looks like that needed ~three hours, and stayed pretty constant 
## at 60gig of RAM. Quite intensive. 

## hopefully this data object is compatible.
## get it from the lab server:

/media/vol1/daniel/franzMetabarcode/CO1_assTax.rds
getFile="/media/vol1/daniel/franzMetabarcode/CO1_assTax.rds"
putItHere="/media/vol/franzData/rawReadsReorganized/"
scp test@132.180.112.115:$getFile $putItHere 

## back to R on local computer

seqtab.nochim <- readRDS("seqtab_nochim_CO1.rds")

taxa <- readRDS("CO1_assTax.rds")

taxa.print <- taxa  # Removing sequence rownames for display only

rownames(taxa.print) <- NULL

tail(taxa.print) 

samples.out <- rownames(seqtab.nochim)

getSampNu <- function(x){paste0("S", as.character(as.integer(strsplit(x, "_")[[1]][1]) - 96))}

## use these as new rownames
rownames(seqtab.nochim) <- sapply(samples.out, getSampNu)

seqtab.nochim[1:5,1:2]

## make our phyloseq object:

ps <- phyloseq(otu_table(seqtab.nochim, taxa_are_rows=FALSE), 
               tax_table(taxa))

dna <- Biostrings::DNAStringSet(taxa_names(ps))
names(dna) <- taxa_names(ps)
ps <- merge_phyloseq(ps, dna)

taxa_names(ps) <- paste0("ASV", seq(ntaxa(ps)))

otu_table(ps)[1:5,1:5]

sample_names(ps)

## this all looks great, but the taxanomic assignments are
## horrible:

tax_table(ps)[1:3,]

## most assignments are NAs, even at high levels:

sum(is.na(tax_table(ps)[,"Kingdom"])) ## out of 6568 OTUs, 3072 are not identified to kingdom. The ones that are 

sum(is.na(tax_table(ps)[,"Phylum"])) ## out of 6568 OTUs, 3783 are not identified

plot_bar(ps, fill = "Phylum")

tax_table(ps)

tax_table(ps)[,"Phylum"]

tax_table(ps)$Phylum

## the column names are off, they start with phylum:

colnames(tax_table(ps))

## easy fix
colnames(tax_table(ps)) <- c("Phylum", "Class", "Order", "Family", "Genus", "Species")


## looks like we need to try a different method. Maybe try a blast-based method?

## export our rep sequences:

writeXStringSet(refseq(ps), "franzCO1_repset.fa")

## while we're at it, export this highly flawed but reformatted taxa that 
## should slide right back into the phyloseq object, should be we need it:

write.csv(tax_table(ps), file="rdpClassifier_CO1_taxonomy.csv")

## test this
tryoutcsv <- read.csv("rdpClassifier_CO1_taxonomy.csv", row.names=1)

head(tryoutcsv)

tax_table(ps) <- as.matrix(minimapTaxAssign)

## a lot (the majority?) of folks are still using blast-based methods.
## the midori folks offer a blast-formatted dataset, whatever that means:

cd /media/vol/franzData/midoriDB

wget https://www.reference-midori.info/download/Databases/GenBank260_2024-04-15/BLAST/uniq/MIDORI2_UNIQ_NUC_GB260_CO1_BLAST.zip

## this is a compiled blastn database

blastnDatabase="/media/vol/franzData/midoriDB/blast/MIDORI2_UNIQ_NUC_GB260_CO1_BLAST"
blastn -task blastn  -query franzCO1_repset.fa \
    -db $blastnDatabase \
    -num_threads 3 \
    -num_alignments 1 \
    -outfmt 10 \
    -out "franzCO1_midoriBlastTopMatches.csv" &

## ugh, some kind of version error. 
## let's try minimap2. 
## we can probably use the existing CO1 database formatted for dada2:

minimap2 -d MIDORI2_UNIQ_NUC_GB260_CO1_DADA2.mmi MIDORI2_UNIQ_NUC_GB260_CO1_DADA2.fasta

mmi=/media/vol/franzData/midoriDB/MIDORI2_UNIQ_NUC_GB260_CO1_DADA2.mmi
reads=/media/vol/franzData/rawReadsReorganized/franzCO1_repset.fa
minimap2 \
  --secondary=no \
  $mmi $reads \
  1> franzCO1_repset_taxAssignments.paf \
  2> franzCO1_repset_taxAssignments.log &

## we want the first column (ASV number), the 6th column of taxonomy, and the last (12) column, the mapQ values

cut -f1,6,12 franzCO1_repset_taxAssignments.paf > minimap2taxonomyWithMappingQualities.tsv
cut -f1,6 franzCO1_repset_taxAssignments.paf > minimap2taxonomyNoMappingQualities.tsv


colnames(tax_table(ps)) <- c("Phylum", "Class", "Order", "Family", "Genus", "Species")

## so, let's generate three CSVs:
## 1. The tax table generated by the conservative classifier in DADA2
## 2. The taxonomic table generated by the Midori database and Minimap2, with only highest quality alignments
## 3. The taxonomic table generated by the Midori database and Minimap2, with mapping quality scores and lower score alignments

## the first two should be formatted so as to be easily inserted into a phyloseq object
## the last one is a reference for decision making: how much can an assignment be trusted, and
## alternative possible assignments

## the second one is the trickiest, let's use python/pandas:

conda activate spatialDirt ## conda env from another project

import pandas as pd
import natsort as ns


aa = pd.read_csv("minimap2taxonomyWithMappingQualities.tsv", 
                 sep="\t", 
                 header=None, 
                 names=("ASVnumber","TaxAssignment","MapQ"))

aa.head()

aa.tail()

aa.ASVnumber.duplicated().sum() ## 2812 dups

(~aa.ASVnumber.duplicated()).sum() ## 5470
aa.ASVnumber.nunique() ## some ASVs didn't align to anything and aren't here. 


## which ones are we missing?

## now, for each ASV, find the assignment that matches this max.
## in case of tie, I guess just take the first

maxMapQEachASV = aa.groupby('ASVnumber')['MapQ'].transform("max")

## which rows meet this maxQ? keep these
idx = maxMapQEachASV == aa['MapQ']
cc = aa[idx]

## there are still duplicates, because alignments to different 
## organisms sometimes score equally

cc.ASVnumber.duplicated().sum() ## 817 dups

## let's just keep the first for each match, for now
dd = cc[~cc.ASVnumber.duplicated()]

dd.shape ##5470 - hmm, lost some ASV names, because there was no match

dd.set_index('ASVnumber', inplace=True)

## can we restore these?
otuNames= [ "ASV"+str(i) for i in range(1,6569) ]
ee = pd.DataFrame(index=otuNames, columns=['TaxAssignment', 'MapQ'])
ee.fillna(dd, inplace=True)

## can we split up this Taxonomic assignment to columns?
ff = ee['TaxAssignment'].str.split(pat="_.*?;", expand=True, regex=True)
ff.columns=["Phylum", "Class", "Order", "Family", "Genus", "Species"]
## add the mapq scores back in:
ff['MapQ'] = ee['MapQ'] 
ff.reset_index(inplace=True)
ff.rename({'index':'ASVnumber'}, axis='columns', inplace=True)
 
## looks okay. Now we need to get these taxonomy tables to R and phyloseq:

## one with quality scores:
ff.to_csv("minimap2taxAssignments_cleanedup.csv", index=False, na_rep="NA")

## one without, for direct import to phyloseq:
gg = ff.drop('MapQ', axis=1)
gg.to_csv("minimap2taxAssignments_forPhyloseq.csv", index=False, na_rep="NA")

#### back in R ####

## import without the MapQ scores
minimapTaxAssign <- read.csv('minimap2taxAssignments_forPhyloseq.csv', row.names="ASVnumber")

aa <- read.csv("minimap2taxAssignments_cleanedup.csv", row.names=1)

## remove the mapq column:
head(minimapTaxAssign)

## will phyloseq accept this? 
tax_table(ps) <- as.matrix(minimapTaxAssign)

## works. Update the repo with these two versions. And the original RDP/DADA2 object,
## which might be interesting. Code added above

## these are the three taxonomy objects of interest

'minimap2taxAssignments_cleanedup.csv'
'minimap2taxAssignments_forPhyloseq.csv'
'rdpClassifier_CO1_taxonomy.csv'

## more descriptive name
mv 'minimap2taxAssignments_cleanedup.csv' 'minimap2taxAssignments_with_MapQ.csv'

