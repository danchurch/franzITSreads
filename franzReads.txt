## let's see what Franz's reads look like

## put his reads on our beastly, old, slow drive
mv /home/daniel/Downloads/Raw.zip /media/vol/franzData/

unzip Raw.zip

## what are the primers?

## GATGAAGAACGYAGYRAA
## RBTTTCTTTTCCTCCGCT

##     GATGAAGAACG Y  AG Y   R  AA

fprim='GATGAAGAACG[CT]AG[CT][AG]AA'

## the full IUPAC reverse complement would be:
'TTYRCTRCGTTCTTCATC'
## so...
fprimRC='TT..CT.CGTTCTTCATC'

##      R    B  TTTCTTTTCCTCCGCT
rprim='[AG][GCT]TTTCTTTTCCTCCGCT'

## the full reverse complement IUPAC would be:

## or
#rprim='..TTTCTTTTCCTCCGCT'

rprimRC='AGCGGAGGAAAAGAAA..'

## look for these
cd /media/vol/franzData/Raw/FITS2/EC_2021_080_057_F_ITS_E192_L001-ds.c5b0e235b400480c9f7e35d36a59096f

zgrep --color $rprim *R1*
zgrep --color $rprim *R2*
zgrep --color $fprim *R1*
zgrep --color $fprim *R2*

## seems consistent, assume this is the case for all the files, R2 has the forward primer-linked sequences,
## R1 has the reverse.

## let's try cutadapt.

## following the cutadapt manual (https://cutadapt.readthedocs.io/en/stable/recipes.html)

conda activate cutadapt

cd /media/vol/franzData/Raw/FITS2
outdir="/media/vol/franzData/trimmedReads/"
#mkdir $outdir
for i in *; do
  cd $i
  echo $PWD
  ls
  R1=$(ls *R1*.fastq.gz)
  R2=$(ls *R2*.fastq.gz)
  echo ${R1} ${R2}
  echo $outdir${R1} $outdir${R2}
  cutadapt -a RBTTTCTTTTCCTCCGCT...TTYRCTRCGTTCTTCATC \
           -A GATGAAGAACGYAGYRAA...AGCGGAGGAAAAGAAAVY \
           --discard-untrimmed \
           --match-read-wildcards \
           -o $outdir${R1} \
           -p $outdir${R2} \
           $R1 $R2
  cd ..
  echo $PWD
  echo "#########################################################"
done

## that gives some empty reads, probably some floating primers
## get rid of these:


cd $outdir
for i in *; do
  echo $i
  seqtk seq -L 50 $i | gzip > ${i}_noshorts
  mv ${i}_noshorts $i
  ls $i
done

## example 

## did that work? Let's assume it did, not much time. 

## moving on with dada2:

R

library(dada2)
library(ShortRead)
library(Biostrings)

plotQualityProfile("/media/vol/franzData/trimmedReads/331_S331_L001_R2_001.fastq.gz")

plotQualityProfile("/media/vol/franzData/trimmedReads/", aggregate=TRUE)


## okay, works. That's what they asked for but now they want to
## run through full dada2 pipeline with me

## that's more work. Let's start a repo:

git clone https://github.com/danchurch/franzITSreads.git

## test

touch thisIsNotReal.txt

git config --global user.email "danchurchthomas@gmail.com"
git config --global user.name "danchurch"
git remote set-url origin git@github.com:danchurch/franzITSreads.git

## so, franz would love it if everything were done in R.



## anyway they have an example script:
https://benjjneb.github.io/dada2/ITS_workflow.html

## they want all files in the same directory. Franz said to take files
## only up to a point, the first 39 plots or so


cd /media/vol/franzData/Raw/FITS2

## just to understand the file names:

## each pair is in a folder
EC_2021_080_001_FITS_E176_L001-ds.b363ac7ea0564138bb1452757e403ce7 ## first samples
## the filenames then look like this:
193_S193_L001_R1_001.fastq.gz  193_S193_L001_R2_001.fastq.gz

EC_2021_080_039_FITS_E176_L001-ds.977b416e0afc478fa6a786e82c4370e3 ## last paired samples, #39
231_S231_L001_R1_001.fastq.gz  231_S231_L001_R2_001.fastq.gz

## so the information we are interested in is in the folder names, not the read files
## the sample number is the first number after "080" in the folder name. 
## but it looks like we can recover the sample from the sequencer sample number in the read file names,
## by subtracting 192 from the first number in each read name.

## so okay to combine them all into a folder, we can recover the sample from them.

cd /media/vol/franzData/Raw/FITS2

for i in {1..9}; do
  echo $i
  cp EC_2021_080_00${i}_FITS_E176_L001-ds.*/* /media/vol/franzData/rawReadsReorganized/
  #cd ..
done

for i in {10..39}; do
  echo $i
  cp EC_2021_080_0${i}_FITS_E176_L001-ds.*/* /media/vol/franzData/rawReadsReorganized/
done


## from there I think we can follow the tutorial in R
https://benjjneb.github.io/dada2/ITS_workflow.html

library(dada2)
library(ShortRead)
library(Biostrings)
library(phyloseq)
library(ggplot2)

path <- "/media/vol/franzData/rawReadsReorganized" 

list.files(path)

## above we noted that the sequences with the forward primer are the R2:
fnFs <- sort(list.files(path, pattern = "R2_001.fastq.gz", full.names = TRUE))
fnRs <- sort(list.files(path, pattern = "R1_001.fastq.gz", full.names = TRUE))

## primer sequences, with IUPAC ambiguous bases:
FWD <- "GATGAAGAACGYAGYRAA"  
REV <- "RBTTTCTTTTCCTCCGCT"  

allOrients <- function(primer) {
    # Create all orientations of the input sequence
    require(Biostrings)
    dna <- DNAString(primer)  # The Biostrings works w/ DNAString objects rather than character vectors
    orients <- c(Forward = dna, Complement = Biostrings::complement(dna), Reverse = Biostrings::reverse(dna),
        RevComp = Biostrings::reverseComplement(dna))
    return(sapply(orients, toString))  # Convert back to character vector
}

FWD.orients <- allOrients(FWD)
REV.orients <- allOrients(REV)

FWD.orients
REV.orients

fnFs.filtN <- file.path(path, "filtN", basename(fnFs)) # Put N-filtered files in filtN/ subdirectory
fnRs.filtN <- file.path(path, "filtN", basename(fnRs))

## get rid of N-labeled nucleotides
filterAndTrim(fnFs, fnFs.filtN, fnRs, fnRs.filtN, maxN = 0, multithread = TRUE)

## look for our primers in the reads, using essentially the vcountPattern function from biostrings
primerHits <- function(primer, fn) {
    # Counts number of reads in which the primer is found
    nhits <- vcountPattern(primer, sread(readFastq(fn)), fixed = FALSE)
    return(sum(nhits > 0))
}

## make a table of which primers are found where. 

rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs.filtN[[1]]), 
      FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = fnRs.filtN[[1]]), 
      REV.ForwardReads = sapply(REV.orients, primerHits, fn = fnFs.filtN[[1]]), 
      REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs.filtN[[1]]))

## makes sense. Forward primers are where they expected, RC primers same

## check cut adapt path. Might be weird because I'm in a conda env
cutadapt <- "/home/daniel/miniconda3/envs/cutadapt/bin/cutadapt" # cutadapt path, in my case in a conda path
system2(cutadapt, args = "--version") # Run shell commands from R

## start prepping the command for cutadapt:

path.cut <- file.path(path, "cutadapt")

if(!dir.exists(path.cut)) dir.create(path.cut)
fnFs.cut <- file.path(path.cut, basename(fnFs))
fnRs.cut <- file.path(path.cut, basename(fnRs))


FWD.RC <- dada2:::rc(FWD)
REV.RC <- dada2:::rc(REV)
# Trim FWD and the reverse-complement of REV off of R1 (forward reads)
R1.flags <- paste("-g", FWD, "-a", REV.RC) 
# Trim REV and the reverse-complement of FWD off of R2 (reverse reads)
R2.flags <- paste("-G", REV, "-A", FWD.RC) 


# Run Cutadapt
for(i in seq_along(fnFs)) {
  system2(cutadapt, args = c(R1.flags, R2.flags, "-n", 2, # -n 2 required to remove FWD and REV from reads
                             "-o", fnFs.cut[i], "-p", fnRs.cut[i], # output files
                             "-m", 20, # enforce a minimum length
                             fnFs.filtN[i], fnRs.filtN[i])) # input files
}

## check that it worked:

rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs.cut[[1]]), 
      FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = fnRs.cut[[1]]), 
      REV.ForwardReads = sapply(REV.orients, primerHits, fn = fnFs.cut[[1]]), 
      REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs.cut[[1]]))

## gone, awesome

# Forward and reverse fastq filenames have the format:
cutFs <- sort(list.files(path.cut, pattern = "R2_001.fastq.gz", full.names = TRUE))
cutRs <- sort(list.files(path.cut, pattern = "R1_001.fastq.gz", full.names = TRUE))

# Extract sample names, for the moment let's use the sample numbers given by the sequencer

get.sample.name <- function(fname) strsplit(basename(fname), "_")[[1]][1]
sample.names <- unname(sapply(cutFs, get.sample.name))

plotQualityProfile(cutFs[1:2])
## huh, these don't look amazing. At least they have enough depth.

################################################################################
## side note, I don't really like their plotter. 
## Let's look at these outside of R in good old fashioned fastqc 
cp /media/vol/franzData/rawReadsReorganized/cutadapt/* /media/vol/franzData/rawReadsReorganized/concatted4fastqc/
cd /media/vol/franzData/rawReadsReorganized/concatted4fastqc/
gunzip *
cat *R1* | gzip > oneBigFastqFranzITS_R1_reads.fastq.gz
cat *R2* | gzip > oneBigFastqFranzITS_R2_reads.fastq.gz
rm *fastq
fastqc oneBigFastqFranzITS_R1_reads.fastq.gz -o /media/vol/franzData/franzITSreads/
fastqc oneBigFastqFranzITS_R2_reads.fastq.gz -o /media/vol/franzData/franzITSreads/
cd /media/vol/franzData/franzITSreads
firefox *html
## okay, that is nice to look at. 
################################################################################

## anyway, on down the pipeline

filtFs <- file.path(path.cut, "filtered", basename(cutFs))
filtRs <- file.path(path.cut, "filtered", basename(cutRs))

## on the tutorial they do an additional filtering step. Don't know why
## they don't do this earlier.

## the settings used in the tutorial for the 
## following command cause the vast majority to
## be thrown out. So I relaxed the max expected
## error from 2 to 5. We still lose average of ~50%

out <- filterAndTrim(cutFs, filtFs, cutRs, filtRs, 
    maxN = 0, 
    maxEE = c(5, 5), 
    truncQ = 2,
    minLen = 50, 
    rm.phix = TRUE,
    compress = TRUE, 
    multithread = TRUE)  

out

colSums(out) ## 1416136 in, 815679 out

errF <- learnErrors(filtFs, multithread = TRUE)
errR <- learnErrors(filtRs, multithread = TRUE)

plotErrors(errF, nominalQ = TRUE)

plotErrors(errR, nominalQ = TRUE)

## try pooled. If it takes to long, move to lab computer?
dadaFs <- dada(filtFs, err = errF, multithread = TRUE, pool=TRUE) ## ~20 min, 10 gig RAM
dadaRs <- dada(filtRs, err = errR, multithread = TRUE, pool=TRUE) ##

dadaRs

mergers <- mergePairs(dadaFs, filtFs, dadaRs, filtRs, verbose=TRUE)

seqtab <- makeSequenceTable(mergers)

?makeSequenceTable

dim(seqtab)

seqtab[1:3,1:2]

sum(seqtab) ##560718 reads. Seems low.

seqtab.nochim <- removeBimeraDenovo(seqtab, method="consensus", multithread=TRUE, verbose=TRUE)

## check the variability in sequence length:
hist(nchar(getSequences(seqtab.nochim)), breaks=20)

getN <- function(x) sum(getUniques(x))

track <- cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN), sapply(mergers, getN),
    rowSums(seqtab.nochim))

colnames(track) <- c("input", "filtered", "denoisedF", "denoisedR", "merged", "nonchim")
rownames(track) <- sample.names

track

## side note, prepping for taxonomic id

## I downloaded UNITE sh_general_release_04.04.2024.tgz
## as available from:
## https://doi.plutof.ut.ee/doi/10.15156/BIO/2959332
## https://doi.org/10.15156/BIO/2959332

## the general page is here: https://unite.ut.ee/repository.php

## this is version 10.0, dynamic, non-dev version with fewer singletons.

unite.ref <- "/media/vol/franzData/sh_general_release_dynamic_04.04.2024.fasta"  # CHANGE ME to location on your machine

## start time is 2024-07-29 17:10
taxa <- assignTaxonomy(seqtab.nochim, unite.ref, multithread = TRUE, tryRC = TRUE)

print(paste("finish time is", Sys.time())) ## that took maybe 15 min, 15 gig RAM

## okay, finish this up, get to a phyloseq object with rep seqs and taxonomy and metadata

taxa.print <- taxa  # Removing sequence rownames for display only
rownames(taxa.print) <- NULL

head(taxa.print) 

## we don't have a metadata file, but can get some information from 
## your 
samples.out <- rownames(seqtab.nochim)

## theoretically, we can just take the order of these samples as their sample number,
## but not very robust, so make a function that gets the sample out of the name:

getSampNu <- function(x){paste0("S", as.character(as.integer(strsplit(x, "_")[[1]][1]) - 192))}

## use these as new rownames
rownames(seqtab.nochim) <- sapply(samples.out, getSampNu)

seqtab.nochim[1:5,1:2]

## make our phyloseq object:
ps <- phyloseq(otu_table(seqtab.nochim, taxa_are_rows=FALSE), 
               tax_table(taxa))

## we would normally add environmental data here

## change out the repseqs for ASV name/numbers

dna <- Biostrings::DNAStringSet(taxa_names(ps))
names(dna) <- taxa_names(ps)
ps <- merge_phyloseq(ps, dna)
taxa_names(ps) <- paste0("ASV", seq(ntaxa(ps)))
otu_table(ps)[1:5,1:5]

## should be ready for ordinations, etc. But we need environmental data. 


############## co1 pipeline ####################

## I signed to make the co1 pipeline community matrix

## seems like this is the paper to look at:
https://www.nature.com/articles/s41598-018-22505-4     
## (Porter and Hajibabaei 2018)

## github repo here: 

## dada3 github issue mentioning this tool
https://github.com/benjjneb/dada2/issues/922

## looks like this will be a task. General approach would be
## get the denoising then branch off to other software
## for the taxonomic identification, then move back into 
## pipeline for phyloseq object:

## first step, as always, find the primers:

## forward:
GGWACWGGWTGAACWGTWTAYCCYCC

## reverse
TAAACTTCAGGGTGACCAAARAAYCA

## greppable:
fprim="GG.AC.GG.TGAAC.GT.TA.CC.CC"
rprim="TAAACTTCAGGGTGACCAAA.AA.CA"

cd /media/vol/franzData/Raw/CO1

cd EC_2021_080_048_L001-ds.06b957a958394972b9c8b8ef19a6b385

zgrep --color $fprim *R1*  ## nope
zgrep --color $rprim *R1* ## lots
zgrep --color $fprim *R2* ## lots
zgrep --color $rprim *R2* ## one, weird

## so, 


## as before, flatten directory, again only the first 39 samples:

## empty our old working directory and start again CO1 reads

#rm -r /media/vol/franzData/rawReadsReorganized/
mkdir -p /media/vol/franzData/rawReadsReorganized/

cd /media/vol/franzData/Raw/CO1

for i in {1..9}; do
  echo $i
  cp EC_2021_080_00${i}_CO1_E176_L001-ds.*/* /media/vol/franzData/rawReadsReorganized/
done

for i in {10..39}; do
  echo $i
  cp EC_2021_080_0${i}_CO1_E176_L001-ds.*/* /media/vol/franzData/rawReadsReorganized/
done

cd /media/vol/franzData/rawReadsReorganized/

## check again with all sequences
zgrep -c --color $fprim *R1*  ## nope
zgrep -c --color $rprim *R1* ## lots
zgrep -c --color $fprim *R2* ## lots
zgrep -c --color $rprim *R2* ## one, weird

## and then back on the R pipeline:

library(dada2)
library(ShortRead)
library(Biostrings)
library(phyloseq)
library(ggplot2)


path <- "/media/vol/franzData/rawReadsReorganized" 

list.files(path)

## above we noted that the sequences with the forward primer are the R2:

fnFs <- sort(list.files(path, pattern = "R2_001.fastq.gz", full.names = TRUE))

fnRs <- sort(list.files(path, pattern = "R1_001.fastq.gz", full.names = TRUE))

FWD <- "GGWACWGGWTGAACWGTWTAYCCYCC"  
REV <- "TAAACTTCAGGGTGACCAAARAAYCA"  

allOrients <- function(primer) {
    # Create all orientations of the input sequence
    require(Biostrings)
    dna <- DNAString(primer)  # The Biostrings works w/ DNAString objects rather than character vectors
    orients <- c(Forward = dna, Complement = Biostrings::complement(dna), Reverse = Biostrings::reverse(dna),
        RevComp = Biostrings::reverseComplement(dna))
    return(sapply(orients, toString))  # Convert back to character vector
}

fnFs.filtN <- file.path(path, "filtN", basename(fnFs)) # Put N-filtered files in filtN/ subdirectory
fnRs.filtN <- file.path(path, "filtN", basename(fnRs))


## get rid of N-labeled nucleotides
filterAndTrim(fnFs, fnFs.filtN, fnRs, fnRs.filtN, maxN = 0, multithread = TRUE)

## look for our primers in the reads, using essentially the vcountPattern function from biostrings
primerHits <- function(primer, fn) {
    # Counts number of reads in which the primer is found
    nhits <- vcountPattern(primer, sread(readFastq(fn)), fixed = FALSE)
    return(sum(nhits > 0))
}


## make a table of which primers are found where. 

FWD.orients <- allOrients(FWD)
REV.orients <- allOrients(REV)

rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs.filtN[[1]]), 
      FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = fnRs.filtN[[1]]), 
      REV.ForwardReads = sapply(REV.orients, primerHits, fn = fnFs.filtN[[1]]), 
      REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs.filtN[[1]]))

## check cut adapt path. Might be weird because I'm in a conda env
cutadapt <- "/home/daniel/miniconda3/envs/cutadapt/bin/cutadapt" # cutadapt path, in my case in a conda path
system2(cutadapt, args = "--version") # Run shell commands from R

## start prepping the command for cutadapt:

path.cut <- file.path(path, "cutadapt")

if(!dir.exists(path.cut)) dir.create(path.cut)
fnFs.cut <- file.path(path.cut, basename(fnFs))
fnRs.cut <- file.path(path.cut, basename(fnRs))

FWD.RC <- dada2:::rc(FWD)
REV.RC <- dada2:::rc(REV)

# Trim FWD and the reverse-complement of REV off of R1 (forward reads)
R1.flags <- paste("-g", FWD, "-a", REV.RC) 
# Trim REV and the reverse-complement of FWD off of R2 (reverse reads)
R2.flags <- paste("-G", REV, "-A", FWD.RC) 

# Run Cutadapt
for(i in seq_along(fnFs)) {
  system2(cutadapt, args = c(R1.flags, R2.flags, "-n", 2, # -n 2 required to remove FWD and REV from reads
                             "-o", fnFs.cut[i], "-p", fnRs.cut[i], # output files
                             "-m", 20, # enforce a minimum length
                             "-j", 0, ## auto-detect cores
                             "--discard-untrimmed", ## there are 1-2 reads that appear flipped
                             fnFs.filtN[i], fnRs.filtN[i])) # input files
}

## check that it worked:
rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs.cut[[1]]), 
      FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = fnRs.cut[[1]]), 
      REV.ForwardReads = sapply(REV.orients, primerHits, fn = fnFs.cut[[1]]), 
      REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs.cut[[1]]))

## looks clean. 

cutFs <- sort(list.files(path.cut, pattern = "R2_001.fastq.gz", full.names = TRUE))
cutRs <- sort(list.files(path.cut, pattern = "R1_001.fastq.gz", full.names = TRUE))

# Extract sample names, for the moment let's use the sample numbers given by the sequencer
get.sample.name <- function(fname) strsplit(basename(fname), "_")[[1]][1]
## looks like these sequencer names are the Franz-sample number + 96 

sample.names <- unname(sapply(cutFs, get.sample.name))

plotQualityProfile(cutFs[1:2])

pdf('forwardR2readQualitys.pdf')
plotQualityProfile(cutFs[1:12])
dev.off()
## the R1 (reverse) look much better
pdf('reversR1readQualitys.pdf')
plotQualityProfile(cutRs[1:12])
dev.off()

## there is a LOT of variablility of quality among samples. Some are fine,
## others are really nasty. 
## but I always feel this way when I use the dada2 plotter
## look at them with fastqc, and it doesn't looks so bad...

################################################################################
## Let's look at these outside of R in good old fashioned fastqc 
mkdir /media/vol/franzData/rawReadsReorganized/concatted4fastqc/
cp /media/vol/franzData/rawReadsReorganized/cutadapt/* /media/vol/franzData/rawReadsReorganized/concatted4fastqc/
cd /media/vol/franzData/rawReadsReorganized/concatted4fastqc/
gunzip *
cat *R1* | gzip > oneBigFastqFranzCO1_R1_reads.fastq.gz
cat *R2* | gzip > oneBigFastqFranzCO1_R2_reads.fastq.gz
rm *fastq
mkdir /media/vol/franzData/franzCO1reads/
fastqc oneBigFastqFranzCO1_R1_reads.fastq.gz -o /media/vol/franzData/franzCO1reads/
fastqc oneBigFastqFranzCO1_R2_reads.fastq.gz -o /media/vol/franzData/franzCO1reads/
cd /media/vol/franzData/franzITSreads
firefox *html
################################################################################

filtFs <- file.path(path.cut, "filtered", basename(cutFs))
filtRs <- file.path(path.cut, "filtered", basename(cutRs))

## let's get rid R2 (forward, "Fs") reads after 250 BP, the quality is pretty bad after that.
## these reads are lower quality, it is hard to enforce any quality cutoffs
## without losing astronomical amounts of reads...

## the target CO1 region is 313 BP, so we can trim a lot, if this is helpful

out <- filterAndTrim(cutFs, filtFs, cutRs, filtRs, 
    maxN = 0, 
    truncQ = 5,
    truncLen = c(210,260),
    minLen = 50, 
    rm.phix = TRUE,
    compress = TRUE, 
    multithread = TRUE)  


plotQualityProfile(cutFs, aggregate=TRUE)

dev.new()
plotQualityProfile(filtFs, aggregate=TRUE)

## for reverse/R1 reads, keep up to 260 bp:

plotQualityProfile(cutRs, aggregate=TRUE)

plotQualityProfile(filtRs, aggregate=TRUE)

## looks good.

colSums(out) ## 1386093   1382028

## assuming that worked, we need a script,
## because this will need to run through to finish
## while I am away...

print(Sys.time())
errF <- learnErrors(filtFs, multithread = TRUE)
print(Sys.time())
print("########################################################")
print(Sys.time())
errR <- learnErrors(filtRs, multithread = TRUE)
print(Sys.time())

save(errF, file="errF_CO1.rda")
save(errR, file="errR_CO1.rda")

plotErrors(errF, nominalQ = TRUE)

plotErrors(errR, nominalQ = TRUE)

#### finishCO1dada2.r #####

library(dada2)
library(ShortRead)
library(Biostrings)
library(phyloseq)
library(ggplot2)
load(file="/media/vol/franzData/rawReadsReorganized/errF_CO1.rda")
load(file="/media/vol/franzData/rawReadsReorganized/errR_CO1.rda")
setwd("/media/vol/franzData/rawReadsReorganized")
path <- "/media/vol/franzData/rawReadsReorganized"
path.cut <- file.path(path, "cutadapt")
cutFs <- sort(list.files(path.cut, pattern = "R2_001.fastq.gz", full.names = TRUE))
cutRs <- sort(list.files(path.cut, pattern = "R1_001.fastq.gz", full.names = TRUE))
filtFs <- file.path(path.cut, "filtered", basename(cutFs))
filtRs <- file.path(path.cut, "filtered", basename(cutRs))
print(Sys.time())
dadaFs <- dada(filtFs, err = errF, multithread = TRUE, pool=TRUE) 
print(Sys.time())
print("########################################################")
print(Sys.time())
dadaRs <- dada(filtRs, err = errR, multithread = TRUE, pool=TRUE) 
print(Sys.time())
save(dadaFs, file="dadaFs_CO1.rda")
save(dadaRs, file="dadaRs_CO1.rda")
mergers <- mergePairs(dadaFs, filtFs, dadaRs, filtRs, verbose=TRUE)
save(mergers, file="mergersCO1.rda")

#### finishCO1dada2.r #####

nohup Rscript finishCO1dada2.r &> finishCO1dada2.log &  ## 


